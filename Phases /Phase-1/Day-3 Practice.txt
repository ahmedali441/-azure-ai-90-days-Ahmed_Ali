ğŸ”µ Lab 3 â€” Token Usage & Cost
â€¢	View metrics
â€¢	Tokens per request
â€¢	Cost analysis
â€¢	Budgets + alerts


This lab is 100% enterprise-critical and shows up a lot in AI-102 thinking.
Weâ€™ll cover:
âœ… Where tokens appear
âœ… How to see usage
âœ… Cost analysis
âœ… Budgets & alerts
âœ… Per-deployment monitoring
Slow + clean ğŸ‘‡
________________________________________
ğŸ§ª Lab-3 Step-by-Step
________________________________________
ğŸ”µ Step 1 â€” Generate Some Traffic
In Chat Playground, run a few prompts:
Short one:
What is churn?
Long one:
Explain in detail why telecom companies suffer from churn and list 5 mitigation strategies.
Run each multiple times.
ğŸ‘‰ This creates token usage for us to inspect.
________________________________________
________________________________________
ğŸ”µ Step 2 â€” View Token Metrics
Go to:
Azure Portal â†’ Your Azure OpenAI resource â†’ Monitoring â†’ Metrics
Add metrics:
â€¢ Processed Prompt Tokens
â€¢ Generated Completion Tokens
â€¢ Total Tokens
Set:
â€¢ Scope = last 30 minutes
â€¢ Aggregation = Sum
You should now see graphs moving ğŸ“ˆ
________________________________________
________________________________________
ğŸ”µ Step 3 â€” Per-Deployment Usage
Still in Metrics:
Change Split by â†’ Deployment name
Now you can see:
âœ” Chat model usage
âœ” Embedding model usage
âœ” Which one costs more
This is very exam-relevant.
________________________________________
________________________________________
ğŸ”µ Step 4 â€” Cost Analysis
Go to:
Azure Portal â†’ Cost Management + Billing â†’ Cost analysis
Filter:
â€¢ Resource = your Azure OpenAI resource
â€¢ Or Resource group
View:
â€¢ Daily cost
â€¢ Accumulated cost
â€¢ Service name = Azure OpenAI
________________________________________
________________________________________
ğŸ”µ Step 5 â€” Create Budget + Alert
Go to:
Cost Management â†’ Budgets â†’ Add
Create:
â€¢ Name: openai-monthly-cap
â€¢ Amount: something small (ex: $10)
â€¢ Scope = resource group
â€¢ Alert at 50% and 80%
ğŸ“Œ This is what enterprises do before production.
________________________________________
________________________________________
ğŸ”µ Step 6 â€” What REALLY Drives Cost?
Run same prompt but:
â€¢ Max tokens = 300
â€¢ Then Max tokens = 50
Look at token charts again.
You should observe:
ğŸ‘‰ Smaller max tokens â†’ cheaper
ğŸ‘‰ Longer prompt â†’ more prompt tokens
ğŸ‘‰ Higher Top-K in RAG later â†’ more tokens
ğŸ‘‰ Embeddings cost per chunk
________________________________________
________________________________________
ğŸ§  Mini-Checkpoint (Answer quickly)
1ï¸âƒ£ Which is more expensive: prompt tokens or completion tokens?
âœ” Both prompt + completion cost money
ğŸ‘‰ Usually completion tokens dominate in chat-heavy apps.

2ï¸âƒ£ Which deployment consumed more so far: chat or embedding?
âœ” Chat almost always > embeddings in testing.

3ï¸âƒ£ Does temperature change token count directly?
âœ” Correct â€” NO direct effect.
ğŸ‘‰ Output length + max tokens do.

4ï¸âƒ£ What setting limits runaway output cost?
âœ” Max tokens = primary control.
âš  Frequency / presence penalties affect style, not cost much.
________________________________________
________________________________________
â­ï¸ Next Lab:
ğŸ‘‰ ğŸ”µ Lab-4 â€” Embeddings + Simple Vector Store


