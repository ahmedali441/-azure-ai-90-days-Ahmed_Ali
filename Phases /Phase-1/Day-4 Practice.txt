ğŸ”µ Lab 4 â€” Embeddings + Simple Vector Store
â€¢	Generate embeddings
â€¢	Store vectors
â€¢	Similarity search
â€¢	Manual Top-K experiments


This lab teaches the core of RAG before Azure AI Search:
ğŸ‘‰ Convert text â†’ vectors â†’ store â†’ similarity search â†’ retrieve Top-K.
No portals chaos. Very controlled.
________________________________________
ğŸ§ª Lab-4 â€” Embeddings + Simple Vector Store
ğŸ¯ Goals
By end of this lab you will:
âœ” Generate embeddings
âœ” Store vectors manually
âœ” Compute similarity
âœ” Tune Top-K
âœ” See cost + relevance tradeoffs
________________________________________
ğŸ§­ Architecture for this lab
Text â†’ Embedding model â†’ Vector list (memory/file)
Query â†’ Embedding â†’ Similarity â†’ Top-K chunks
No search service yet.
________________________________________
âœ… STEP 1 â€” Pick your embedding deployment
In Azure OpenAI:
You should already have:
â€¢ One chat deployment
â€¢ One embedding deployment
We use the embedding one here.
________________________________________
âœ… STEP 2 â€” Sample documents
Use 3â€“5 short docs like:
Doc1: Customers on month-to-month contracts churn more often.
Doc2: Fiber optic users pay higher monthly charges.
Doc3: Long tenure customers are less likely to churn.
Doc4: Electronic check payment correlates with churn.
Each doc = one chunk.
________________________________________
âœ… STEP 3 â€” Generate embeddings (conceptual API flow)
Every chunk is sent to embedding model:
input: "Customers on month-to-month contracts churn more often."
output: [0.012, -0.44, 0.88, ...]
You now store:
[
  { text: "...", vector: [...] },
  { text: "...", vector: [...] }
]
This is your mini vector DB.
________________________________________
âœ… STEP 4 â€” Store vectors (simple store)
You can:
â€¢ Keep in memory
â€¢ JSON file
â€¢ CSV
â€¢ Python dict
Structure:
{
  id: 1,
  text: "...",
  embedding: [...]
}
________________________________________
âœ… STEP 5 â€” Query time
User asks:
"Which customers are likely to churn?"
You:
1ï¸âƒ£ Embed query
2ï¸âƒ£ Compare with all vectors
3ï¸âƒ£ Compute similarity
4ï¸âƒ£ Sort
5ï¸âƒ£ Pick Top-K
________________________________________
âœ… STEP 6 â€” Similarity metric
Use cosine similarity:
â€¢ Range âˆ’1 â†’ 1
â€¢ Higher = closer meaning
________________________________________
âœ… STEP 7 â€” Top-K experiments
Try:
Top-K	Effect
1	Cheap, risky
3	Best default
10	More cost, more noise
50	ğŸš¨ Expensive + junk
ğŸ“Œ Enterprise default: 3â€“8
________________________________________
âœ… STEP 8 â€” What you just built
You implemented:
âœ” Retrieval
âœ” Ranking
âœ” Cost control
âœ” Grounding foundation
âœ” RAG core logic
________________________________________









ğŸ§  Mini Homework (answer before Lab-5)
Reply with:
1ï¸âƒ£ Why do we embed docs once but queries every time?
Your answer was mostly right, but hereâ€™s the clean enterprise version:
ğŸ‘‰ Docs = stable â†’ embed once â†’ store in vector DB
ğŸ‘‰ Queries = change every request â†’ must embed every time
Because:
â€¢ Saves cost
â€¢ Faster retrieval
â€¢ Production systems canâ€™t re-embed thousands of docs per query
ğŸ“Œ Key phrase:
Documents are static; queries are dynamic

2ï¸âƒ£ What happens if Top-K is too low?
Refined:
â€¢ Might miss the relevant chunk
â€¢ Model hallucinates
â€¢ Poor grounding
Enterprise range stays: 3â€“8

3ï¸âƒ£ Why chunk before embedding?
This one needed fixing ğŸ‘€ â€” good you flagged uncertainty.
Chunking happens BEFORE embedding because:
âœ” Embeddings represent meaning of a single piece of text
âœ” Whole documents are too large
âœ” Retrieval must target specific passages
âœ” Improves precision
âœ” Reduces noise
âœ” Fits context windows later
So the real pipeline is:
Documents
â†’ Chunk
â†’ Embed chunks
â†’ Store vectors

4ï¸âƒ£ Which costs more long-term: embeddings or chat tokens?
â€¢ Embedding cost = one-time per document
â€¢ Chat/completion tokens = every query â†’ forever
So:
ğŸ”¥ LLM inference dominates cost in production, not embeddings.

________________________________________
Next ..Day-5:
ğŸ‘‰ ğŸ”µ Lab-5 â€” RAG Lite (Manual)
(chunking + overlap + grounding).
This is where it gets ğŸ”¥.


### the code you are gonna paste it on python: all you need to attach your api key and endpoint:###
import os
from openai import AzureOpenAI
#Step 1 â€” Connect to Azure OpenAI
endpoint = "Your endpoint"
deployment = "text-embedding-3-small"
api_key = "Your api key" # Replace with your actual key
api_version = "2024-02-01"

# Initialize the client correctly
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key # Use api_key here for the OpenAI SDK
)
#Step 2 â€” Generate embeddings
response = client.embeddings.create(
    input=["Customers on month-to-month contracts churn more often","Fiber optic users pay higher monthly charges","Long tenure customers are less likely to churn"],
    model=deployment
)

for item in response.data:
    length = len(item.embedding)
    print(
        f"data[{item.index}]: length={length}, "
        f"[{item.embedding[0]}, {item.embedding[1]}, "
        f"..., {item.embedding[length-2]}, {item.embedding[length-1]}]"
    )
print(response.usage)

#Step 3 â€” Inspect vector size 1536  
#Step 4 â€” Check token usage Usage(prompt_tokens=25, total_tokens=25) both 3 & 4 will appear as o/p once run the code

#Step 5 - Store vectors in a simple vector store
import numpy as np

documents = [
    "Customers on month-to-month contracts churn more often",
    "Fiber optic users pay higher monthly charges",
    "Long tenure customers are less likely to churn"
]

vectors = [item.embedding for item in response.data]

store = {
    "docs": documents,
    "vectors": np.array(vectors)
}

#Step 6 â€” Embed a query
#Now simulate a user question:
query = "Fiber optic user pay lower than DSL servise?"

query_emb = client.embeddings.create(
    input=query,
    model=deployment
).data[0].embedding

#Step 7 â€” Similarity search (Top-K):
from sklearn.metrics.pairwise import cosine_similarity

scores = cosine_similarity(
    [query_emb],
    store["vectors"]
)[0]

top_k = 1
best = scores.argsort()[-top_k:][::-1]

for i in best:
    print(documents[i], scores[i])


#Step 8 â€” Manual Top-K experiments: Now play : Try top_k = 1 , then 3 , then 5 
#Observe relevance, bserve noise
#Did result quality drop?
#Did irrelevant text appear?
#How many chunks really useful?


#Done ,, Congrats !!



####Output of the code:


[Running] python -u "m:\My roadmap\AZ-AI-102 -Exam\Phase-1\Day-4's code.py"
data[0]: length=1536, [-0.02365204319357872, 0.031928107142448425, ..., -0.04044800624251366, 0.010083314962685108]
data[1]: length=1536, [-0.013941646553575993, -0.01904054544866085, ..., -0.013473665341734886, 0.0131872883066535]
data[2]: length=1536, [-0.0182583536952734, 0.019101696088910103, ..., -0.015011487528681755, -0.00895348098129034]
Usage(prompt_tokens=25, total_tokens=25)
Fiber optic users pay higher monthly charges 0.7668978136354963
Customers on month-to-month contracts churn more often 0.3259808166404879
Long tenure customers are less likely to churn 0.25787065587351116

[Done] exited with code=0 in 7.401 seconds

