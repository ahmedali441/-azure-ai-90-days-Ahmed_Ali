ğŸ”µ Lab 5 â€” RAG Lite (Manual)
â€¢	Upload docs
â€¢	Chunking
â€¢	Overlap
â€¢	Retrieval
â€¢	Prompt grounding
(No Azure AI Search yet â€” simple version.)


Goal:
Take retrieved chunks â†’ inject into prompt â†’ force grounding â†’ return answer + citation.
NO Azure AI Search yet.
NO orchestration frameworks.
Pure Python + Azure OpenAI.

ğŸ§ª What Lab-5 Adds on top of your code
You already finished Steps 1â€“8.
Now we add:
________________________________________
ğŸ”µ Step-9 â€” Build Retrieved Context Block
After your similarity loop:
retrieved_docs = [documents[i] for i in best]

context = "\n".join(
    f"[Doc {idx+1}] {doc}"
    for idx, doc in enumerate(retrieved_docs)
)

print("Retrieved context:\n", context)
This simulates what Azure AI Search would later return.
________________________________________
________________________________________
ğŸ”µ Step-10 â€” Grounded Prompt Template
We now force the model:
â€¢ use ONLY retrieved text
â€¢ refuse if not present
â€¢ cite sources
Add:
system_prompt = """
You are a churn prediction assistant.

Answer ONLY from the provided context.
If the answer is not in the context, say: "Information not available in the provided documents."

Always cite sources like [Doc 1].
"""

user_prompt = f"""
Context:
{context}

Question:
{query}
"""
________________________________________
________________________________________
ğŸ”µ Step-11 â€” Call Chat Model with Grounding
âš ï¸ You must already have a chat deployment (GPT-4o / GPT-35 / etc).
Add:
chat_deployment = "your-chat-deployment-name"

chat_response = client.chat.completions.create(
    model=chat_deployment,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
    max_tokens=200,
)

print(chat_response.choices[0].message.content)
print("Usage:", chat_response.usage)
________________________________________
________________________________________
ğŸ”µ Step-12 â€” Citation Enforcement Test
Change query to something NOT in docs:
query = "Do customers in Europe churn more?"
Expected:
Information not available in the provided documents.
If it hallucinates â†’ grounding failed âŒ.
________________________________________
________________________________________
ğŸ”µ Step-13 â€” Chunking Simulation
Replace documents with longer paragraphs:
documents = [
    "Customers on month-to-month contracts churn more often because they are not locked in...",
    "Fiber optic users typically pay higher monthly charges compared to DSL...",
    "Long tenure customers are less likely to churn because loyalty increases..."
]
Later weâ€™ll automate chunking â€” for now youâ€™re seeing why:
ğŸ‘‰ small chunks = precise retrieval
ğŸ‘‰ big chunks = noise + cost
________________________________________
________________________________________
ğŸ”µ Step-14 â€” Overlap Concept (Manual)
Simulate overlapping chunks:
documents = [
    "Customers on month-to-month contracts churn more often. They are free to cancel anytime.",
    "They are free to cancel anytime and often leave when prices rise.",
]
Notice retrieval improves because overlapping info appears twice.
________________________________________
________________________________________
ğŸ”µ Step-15 â€” Log Tokens for Cost Awareness
You already printed:
Usage(prompt_tokens=XX, completion_tokens=YY)
Write in notebook:
â€¢ context size â†‘ â†’ tokens â†‘
â€¢ Top-K â†‘ â†’ cost â†‘
â€¢ chunk size â†‘ â†’ cost â†‘
________________________________________
________________________________________
âœ… What You Have Now Implemented
You just built:
âœ” Retrieval
âœ” Vector similarity
âœ” Grounding
âœ” Citation forcing
âœ” Hallucination prevention
âœ” Cost trade-offs
âœ” Chunking intuition
âœ” Overlap effect
This is core RAG.
________________________________________
________________________________________
ğŸ§  Why this matters for later
When we move to:
ğŸ‘‰ Azure AI Search
ğŸ‘‰ hybrid search
ğŸ‘‰ HNSW
ğŸ‘‰ filters
ğŸ‘‰ reranker
ALL of that replaces:
store["vectors"]
cosine_similarity()
â€”but the logic stays identical.
________________________________________
________________________________________
ğŸ›‘ Stop Point for Today
When you run this:
â€¢ query
â€¢ retrieval
â€¢ grounded answer
â€¢ citations
â€¢ refusal case
Lab-5 is DONE âœ….
________________________________________
Next...
Start Lab-6 â€” Security Setup
and we continue exactly by the plan.


