ğŸ”µ Lab 5 â€” RAG Lite (Manual)
â€¢	Upload docs
â€¢	Chunking
â€¢	Overlap
â€¢	Retrieval
â€¢	Prompt grounding
(No Azure AI Search yet â€” simple version.)


Goal:
Take retrieved chunks â†’ inject into prompt â†’ force grounding â†’ return answer + citation.
NO Azure AI Search yet.
NO orchestration frameworks.
Pure Python + Azure OpenAI.

ğŸ§ª What Lab-5 Adds on top of your code
You already finished Steps 1â€“8.
Now we add:
________________________________________
ğŸ”µ Step-9 â€” Build Retrieved Context Block
After your similarity loop:
retrieved_docs = [documents[i] for i in best]

context = "\n".join(
    f"[Doc {idx+1}] {doc}"
    for idx, doc in enumerate(retrieved_docs)
)

print("Retrieved context:\n", context)
This simulates what Azure AI Search would later return.
________________________________________
________________________________________
ğŸ”µ Step-10 â€” Grounded Prompt Template
We now force the model:
â€¢ use ONLY retrieved text
â€¢ refuse if not present
â€¢ cite sources
Add:
system_prompt = """
You are a churn prediction assistant.

Answer ONLY from the provided context.
If the answer is not in the context, say: "Information not available in the provided documents."

Always cite sources like [Doc 1].
"""

user_prompt = f"""
Context:
{context}

Question:
{query}
"""
________________________________________
________________________________________
ğŸ”µ Step-11 â€” Call Chat Model with Grounding
âš ï¸ You must already have a chat deployment (GPT-4o / GPT-35 / etc).
Add:
chat_deployment = "your-chat-deployment-name"

chat_response = client.chat.completions.create(
    model=chat_deployment,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
    max_tokens=200,
)

print(chat_response.choices[0].message.content)
print("Usage:", chat_response.usage)
________________________________________
________________________________________
ğŸ”µ Step-12 â€” Citation Enforcement Test
Change query to something NOT in docs:
query = "Do customers in Europe churn more?"
Expected:
Information not available in the provided documents.
If it hallucinates â†’ grounding failed âŒ.
________________________________________
________________________________________
ğŸ”µ Step-13 â€” Chunking Simulation
Replace documents with longer paragraphs:
documents = [
    "Customers on month-to-month contracts churn more often because they are not locked in...",
    "Fiber optic users typically pay higher monthly charges compared to DSL...",
    "Long tenure customers are less likely to churn because loyalty increases..."
]
Later weâ€™ll automate chunking â€” for now youâ€™re seeing why:
ğŸ‘‰ small chunks = precise retrieval
ğŸ‘‰ big chunks = noise + cost
________________________________________
________________________________________
ğŸ”µ Step-14 â€” Overlap Concept (Manual)
Simulate overlapping chunks:
documents = [
    "Customers on month-to-month contracts churn more often. They are free to cancel anytime.",
    "They are free to cancel anytime and often leave when prices rise.",
]
Notice retrieval improves because overlapping info appears twice.
________________________________________
________________________________________
ğŸ”µ Step-15 â€” Log Tokens for Cost Awareness
You already printed:
Usage(prompt_tokens=XX, completion_tokens=YY)
Write in notebook:
â€¢ context size â†‘ â†’ tokens â†‘
â€¢ Top-K â†‘ â†’ cost â†‘
â€¢ chunk size â†‘ â†’ cost â†‘
________________________________________
________________________________________
âœ… What You Have Now Implemented
You just built:
âœ” Retrieval
âœ” Vector similarity
âœ” Grounding
âœ” Citation forcing
âœ” Hallucination prevention
âœ” Cost trade-offs
âœ” Chunking intuition
âœ” Overlap effect
This is core RAG.
________________________________________
________________________________________
ğŸ§  Why this matters for later
When we move to:
ğŸ‘‰ Azure AI Search
ğŸ‘‰ hybrid search
ğŸ‘‰ HNSW
ğŸ‘‰ filters
ğŸ‘‰ reranker
ALL of that replaces:
store["vectors"]
cosine_similarity()
â€”but the logic stays identical.
________________________________________
________________________________________
ğŸ›‘ Stop Point for Today
When you run this:
â€¢ query
â€¢ retrieval
â€¢ grounded answer
â€¢ citations
â€¢ refusal case
Lab-5 is DONE âœ….
________________________________________
########################################### Here the final code starts #########################################

import os
from openai import AzureOpenAI
#Step 1 â€” Connect to Azure OpenAI
endpoint = "Your endpoint"
deployment = "your text embedding choose 'text-embedding-3-small' "
api_key = "Your api-key" # Replace with your actual key
api_version = "2024-02-01"

# Initialize the client correctly
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=api_key # Use api_key here for the OpenAI SDK
)
#Step 2 â€” Generate embeddings
response = client.embeddings.create(
    input=["Customers on month-to-month contracts churn more often","Fiber optic users pay higher monthly charges","Long tenure customers are less likely to churn"],
    model=deployment
)

for item in response.data:
    length = len(item.embedding)
    print(
        f"data[{item.index}]: length={length}, "
        f"[{item.embedding[0]}, {item.embedding[1]}, "
        f"..., {item.embedding[length-2]}, {item.embedding[length-1]}]"
    )
print(response.usage)

#Step 3 â€” Inspect vector size 1536  
#Step 4 â€” Check token usage Usage(prompt_tokens=25, total_tokens=25) both 3 & 4 will appear as o/p once run the code

#Step 5 - Store vectors in a simple vector store
import numpy as np

documents = [
    "Customers on month-to-month contracts churn more often",
    "Fiber optic users pay higher monthly charges",
    "Long tenure customers are less likely to churn"
]

vectors = [item.embedding for item in response.data]

store = {
    "docs": documents,
    "vectors": np.array(vectors)
}

#Step 6 â€” Embed a query
#Now simulate a user question:
query = "Fiber optic user pay lower than DSL servise?"

query_emb = client.embeddings.create(
    input=query,
    model=deployment
).data[0].embedding

#Step 7 â€” Similarity search (Top-K):
from sklearn.metrics.pairwise import cosine_similarity

scores = cosine_similarity(
    [query_emb],
    store["vectors"]
)[0]

top_k = 1
best = scores.argsort()[-top_k:][::-1]

for i in best:
    print(documents[i], scores[i])


#Step 8 â€” Manual Top-K experiments: Now play : Try top_k = 1 , then 3 , then 5 
#Observe relevance, bserve noise
#Did result quality drop?
#Did irrelevant text appear?
#How many chunks really useful?

#Day's 5 lab starts here:

#step 9 Build Retrieved Context Block
retrieved_docs = [documents[i] for i in best]

context = "\n".join(
    f"[Doc {idx+1}] {doc}"
    for idx, doc in enumerate(retrieved_docs)
)

print("Retrieved context:\n", context)

#Step-10 â€” Grounded Prompt Template

system_prompt = """
You are a churn prediction assistant.

Answer ONLY from the provided context.
If the answer is not in the context, say: "Information not available in the provided documents."

Always cite sources like [Doc 1].
"""

user_prompt = f"""
Context:
{context}

Question:
{query}
"""
# Step-11 â€” Call Chat Model with Grounding
chat_deployment = "your chat deployment choose 'chat 40 mini' "

chat_response = client.chat.completions.create(
    model=chat_deployment,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
    max_tokens=200,
)

print(chat_response.choices[0].message.content)
print("Usage:", chat_response.usage)

#Step-12 â€” Citation Enforcement Test

query = "Do customers in Europe churn more?"

#Step-13 â€” Chunking Simulation
documents = [
    "Customers on month-to-month contracts churn more often because they are not locked in...",
    "Fiber optic users typically pay higher monthly charges compared to DSL...",
    "Long tenure customers are less likely to churn because loyalty increases..."
]

#Step-14 â€” Overlap Concept (Manual)
documents = [
    "Customers on month-to-month contracts churn more often. They are free to cancel anytime.",
    "They are free to cancel anytime and often leave when prices rise.",
]

#Step-15 â€” Log Tokens for Cost Awareness, you will see on the o/p:
#Usage(prompt_tokens=XX, completion_tokens=YY)



#Done ,, Congrats !!







#################################################################################################################
Next...
Start Lab-6 â€” Security Setup
and we continue exactly by the plan.


